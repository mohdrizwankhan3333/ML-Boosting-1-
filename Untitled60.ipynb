{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b93061-ace6-45b5-81ec-99f4e07455eb",
   "metadata": {},
   "source": [
    "Q1. What is boosting in machine learning?\n",
    "\n",
    "Boosting is an ensemble technique that sequentially combines weak learners to form a strong learner by focusing on errors of previous models.\n",
    "\n",
    "Q2. What are the advantages and limitations of using boosting techniques?\n",
    "\n",
    "Advantages include high accuracy and handling imbalanced data. Limitations involve risk of overfitting, computational intensity, and complex hyperparameter tuning.\n",
    "\n",
    "Q3. Explain how boosting works.\n",
    "\n",
    "Boosting trains weak learners sequentially, adjusting weights of misclassified instances, and combines their predictions to create a stronger overall model.\n",
    "\n",
    "Q4. What are the different types of boosting algorithms?\n",
    "\n",
    "Common boosting algorithms include AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoost, each with unique enhancements and optimizations.\n",
    "\n",
    "Q5. What are some common parameters in boosting algorithms?\n",
    "\n",
    "Key parameters include learning rate, number of estimators, maximum depth, minimum samples split, and subsample size.\n",
    "\n",
    "Q6. How do boosting algorithms combine weak learners to create a strong learner?\n",
    "\n",
    "Boosting algorithms weight and combine weak learners' predictions, focusing on correcting previous errors to improve overall model performance.\n",
    "\n",
    "Q7. Explain the concept of AdaBoost algorithm and its working.\n",
    "\n",
    "AdaBoost adjusts weights of misclassified samples, trains weak learners sequentially, and combines their weighted predictions to form a strong model.\n",
    "\n",
    "Q8. What is the loss function used in AdaBoost algorithm?\n",
    "\n",
    "AdaBoost typically uses the exponential loss function to measure and minimize classification errors iteratively.\n",
    "\n",
    "Q9. How does the AdaBoost algorithm update the weights of misclassified samples?\n",
    "\n",
    "AdaBoost increases the weights of misclassified samples, making them more influential in subsequent weak learners' training.\n",
    "\n",
    "Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?\n",
    "\n",
    "Increasing the number of estimators generally improves performance but may lead to overfitting if the model becomes too complex."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
